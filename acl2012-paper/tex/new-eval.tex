\section{Evaluation}
\label{sec:eval}

We evaluated our implementation of TrTok compared to other prominent
implementations and approaches to sentence detection. The included
competitors range from the simple rule-based RE system through the
maximum entropy powered approaches of the MxTerminator and of Apache
OpenNLP's SentenceDetector all the way to the unsupervised Punkt
system and the hand-crafted PTBTokenizer and sentence splitter used in
Stanford CoreNLP. The results are presented in
Table~\ref{tbl:grand-melee}.

\subsection{Dataset}

The experiments were conducted on the Brown corpus as is supplied
through NLTK. A representative (covering each category of text
proportionately) 20\% of the corpus was used as the testing data. This
number was chosen so that the testing data would be sure to contain at
least 1000 instances of a non-sentence-terminating full stop, the
resulting test set contains 1481 such full stops. The rest of the data
was made available for training to the supervised learning methods.

\subsection{Sentence Detection Methods}

Absolute Baseline is simply the approach which marks every full stop as
a sentence terminator.

Trtok::Baseline is the simplest tokenizer which can be written in
TrTok. The fact that this performs noticeably better than the Absolute
Baseline is due to the fact that even if we do not specify any
contextual features explicitly, TrTok still passes the built-in
feature 0:\%WHITESPACE to the classifier, which tells it whether the
full stop in question is followed by any whitespace or not. The
classifier has then learned that periods not followed by whitespace
usually do not mean the end of a sentence and uses this to perform
better than the Absolute Baseline.

TrTok::Satz-like is a straightforward attempt at translating the Satz
system to TrTok. The part-of-speech-tagged training data was used to
construct lexicons for each different part of speech tag (NLTK's
method of simplifying tags was used to reduce the number of different
tags to help fight data sparsity). The classifier was then given all
the retrieved part of speech tags for the 3 rough tokens preceding and
succeeding a full stop. In contrast to Satz, our version uses a
different system of tags, a different machine learning algorithm and
most importantly our version does not try to guess the part of speech
tags for words which are not found in the lexicon.

The RE system, MxTerminator and Punkt were described in
Section~\ref{sec:previous-work}. The implementation of RE used for our
experiments was the one provided at
\texttt{http://www.ppgia.pucpr.br/âˆ¼silla/softwares/yasd.zip}, the
implementation of MxTerminator was obtained from
\texttt{ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz} and the
implementation of Punkt is the one from NLTK. As for training, Punkt
received the entire Brown corpus (training data and testing data)
without any annotations while MxTerm was trained using the training
data.

Apache OpenNLP contains a sentence detector based around a maximum
entropy classifier. The implementation is nearly identical to the
specification of MxTerminator with only minor deviations (such as
signalling whitespace around the full stop as features).

We performed experiments both with the ready-to-use sentence detection
model for English distributed via OpenNLP's website and with a model
which was trained on our training data. The parameters for optimally
training the model were estimated using 10-fold cross-validation on
the training data.

The Stanford CoreNLP sentence splitter works by applying its tokenizer
to the input text which will make the distinction between a full stop
as part of an abbreviation or an ordinal number as opposed to being a
sentence terminator. Thus the task of sentence splitting is trivial
after the tokenization has been correctly performed.

The tokenization itself is implemented using a lexical analyzer
generator, JFlex (not unlike how TrTok uses Quex to implement the
RoughTokenizer). The tokenizer itself is a deterministic program
guided by a collection of heuristics.

TrTok::MxTerm-like is a translation of the MxTerminator method to
TrTok. Whereas MxTerm inspects the prefix and suffix of the
full-stop-containing word and the words preceding and succeeding it,
TrTok::MxTerm-like splits the period into its own rough token and
examines the two rough tokens on either side of it. Also,
TrTok::MxTerm-like does

The reason why MxTerminator does not achieve the same performance
could be that the maximum entropy trainer used in MxTerminator limits
itself to 100 iterations of Generalized Iterative Scaling, which
converges very slowly compared to L-BFGS. Another reason might be the
fact that both MxTerminator and OpenNLP use cut off infrequent
features. Also, both OpenNLP and MxTerminator try to induce
abbreviations from training data whereas TrTok::MxTerm-like has a
collection of abbreviations which was obtained by taking the union of
all the different abbreviation lists used in TrTok::Groomed.

Finally, TrTok::Groomed is a relatively large hand-made tokenization
scheme. It includes 7 distinct lists of abbreviations (prefix and
suffix titles, abbreviated names of months\ldots) totalling 303 types,
properties for detecting the case of tokens, for noticing numbers
which happen to be in the range of the days of the month or years,
etc\ldots. These features are extracted from all rough tokens within
the 8 tokens from the full stop and for the 2 closest tokens on either
side of the full stop, the token's type itself is passed as a feature.

Don't forget to mention how the performance measurement were taken!

Placeholder!\cite{sbd-punkt}

\begin{table*}
  \begin{center}
    \begin{tabular}{ | l | c | c | c | c | c | }
      \hline
      & Accuracy $\downarrow$ & Error Rate & Precision
      & Recall & F_1 Measure \\ \hline
      TrTok::Groomed & \textbf{98.87\%} & \textbf{1.12\%} & \textbf{99.13\%}
                     & 99.57\% & \textbf{99.35\%} \\ \hline
      TrTok::MxTerm-like & 98.76\% & 1.24\% & 98.70\%
                         & 99.89\% & 99.29\% \\ \hline
      Punkt & 98.63\% & 1.36\% & 98.81\%
            & 97.35\% & 98.08\% \\ \hline
      Stanford CoreNLP & 98.46\% & 1.53\% & 98.76\%
                       & 99.48\% & 99.12\% \\ \hline
      MxTerminator & 98.10\% & 1.89\% & 98.30\%
                   & 96.68\% & 97.48\% \\ \hline
      Apache OpenNLP & 97.97\% & 2.02\% & 98.20\%
                     & 96.30\% & 97.24\% \\ \hline
      Apache OpenNLP (pre-trained) & 97.47\% & 2.52\% & 98.62\%
                                   & 89.97\% & 94.09\% \\ \hline
      RE & 97.11\% & 2.88\% & 98.66\%
         & 87.33\% & 92.65\% \\ \hline
      TrTok::Satz-like & 96.50\% & 3.49\% & 97.90\%
                      & 98.07\% & 97.99\% \\ \hline
      TrTok::Baseline & 91.84\% & 8.15\% & 91.67\%
                      & 99.66\% & 95.50\% \\ \hline
      Absolute Baseline & 86.89\% & 13.11\% & 86.89\%
                        & \textbf{100.00\%} & 92.99\% \\ \hline
    \end{tabular}
  \end{center}
  \caption[Performance of sentence detectors on the Brown corpus]
    {The performance of the various sentence detectors on the Brown corpus.}
  \label{tbl:grand-melee}
\end{table*}
