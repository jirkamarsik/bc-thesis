\section{Evaluation}
\label{sec:eval}

We evaluated our implementation of TrTok compared to a wide range of
prominent implementations and approaches to sentence detection. The
results are presented in Table~\ref{tbl:grand-melee}.

\begin{table*}
  \begin{center}
    \begin{tabular}{ | l | c | c | c | c | c | r | }
      \hline
      & Accuracy $\downarrow$ & Error Rate & Precision
      & Recall & F_1 Measure & Time \\ \hline
      TrTok::Groomed & \textbf{98.85\%} & \textbf{1.14\%} & \textbf{99.11\%}
                     & 99.57\% & \textbf{99.34\%} & 5.10s \\ \hline
      TrTok::MxTerm-like & 98.76\% & 1.23\% & 98.70\%
                         & 99.88\% & 99.29\% & 1.10s \\ \hline
      TrTok::Easy & 98.69\% & 1.30\% & 98.61\%
                  & 99.90\% & 99.25\% & 1.08s \\ \hline
      Punkt & 98.64\% & 1.35\% & 98.81\%
            & 99.63\% & 99.22\% & 3.13s \\ \hline
      Stanford CoreNLP & 98.53\% & 1.46\% & 98.75\%
                       & 99.57\% & 99.16\% & 7.93s \\ \hline
      MxTerminator & 98.27\% & 1.72\% & 98.30\%
                   & 99.73\% & 99.01\% & 1.37s \\ \hline
      Apache OpenNLP & 98.20\% & 1.79\% & 98.19\%
                     & 99.76\% & 98.97\% & 1.13s \\ \hline
      Apache OpenNLP (pre-trained) & 97.70\% & 2.29\% & 98.61\%
                                   & 98.74\% & 98.68\% & 1.17s \\ \hline
      RE & 97.25\% & 2.74\% & 98.52\%
         & 98.31\% & 98.41\% & 16.93s \\ \hline
      TrTok::Satz-like & 96.50\% & 3.49\% & 97.90\%
                       & 98.07\% & 97.99\% & 1.59s \\ \hline
      TrTok::Baseline & 91.84\% & 8.15\% & 91.67\%
                      & 99.66\% & 95.50\% & 0.85s \\ \hline
      Absolute Baseline & 86.89\% & 13.10\% & 86.89\%
                        & \textbf{100.00\%} & 92.98\% & \textbf{0.02s} \\ \hline
    \end{tabular}
  \end{center}
  \caption[Performance of sentence detectors on the Brown corpus] {The
    performance of the various sentence detectors on full stops from
    the Brown corpus testing data. \\ The 1.15 MB of testing data
    consisted of 11376 sentences and 232893 tokens.}
  \label{tbl:grand-melee}
\end{table*}

\subsection{Dataset}

The experiments were conducted on the Brown corpus \cite{data-brown}
as is supplied through NLTK \cite{software-nltk}. A representative
(covering each category of text proportionately) 20\% of the corpus
was used as the testing data. This number was chosen so that the
testing data would be sure to contain at least 1000 instances of a
non-sentence-terminating full stop; the resulting test set ended up
containing 1481 such full stops. The rest of the data was made
available for training to the supervised learning methods.

\subsection{Performance Measurement}

The performance of the evaluated systems was measured by their success
in classifying instances of the full stop character. The text contains
other sentence terminators such as the question mark and the
exclamation mark, but they almost never figure as anything else but
sentence terminators in the text. Other occasional sentence delimiters
such as dashes, semicolons, colons and line breaks were ignored as
well, since the other systems usually do not have a solution for them.
This way the comparison is fair. Furthermore, the full stop is the
most common and ambiguous of the sentence delimiters.

Besides the systems' accuracy, we also measure the time spent
processing the testing data and we present the median of 11 runs to
bring the implementation speed of the systems into context as well.

\subsection{Sentence Detection Methods}

\textbf{Absolute Baseline} is simply marks every full stop as a
sentence terminator.

\textbf{Trtok::Baseline} is the simplest tokenizer which can be
written in TrTok. The reason that this performs noticeably better than
the Absolute Baseline is that even if we do not specify any contextual
features explicitly, TrTok still passes the built-in feature
\texttt{0:\%WHITESPACE} to the classifier, which tells it whether the
full stop in question is followed by any whitespace or not. The
classifier has then learned that periods not followed by whitespace
usually do not mean the end of a sentence and uses that to perform
better than the Absolute Baseline.

\textbf{TrTok::Satz-like} is a straightforward attempt at translating
the Satz system to TrTok. The POS-tagged training data was used to
construct lexicons for each different part of speech tag (NLTK's
method of simplifying tags was used to reduce the number of different
tags to help fight data sparsity). The classifier was then given all
the retrieved part of speech tags for the 3 rough tokens preceding and
succeeding a full stop.

In contrast to Satz, our version uses a different system of tags, a
different machine learning algorithm and most importantly our version
does not try to guess the part of speech tags for words which are not
found in the lexicon. All of these decisions are for the worse (except
maybe for the choice of machine learning algorithm and hypothesis
representation) and so TrTok::Satz-like does not perform as well as
the original Satz system would have. Sadly, the implementation of Satz
is no longer in the author's possession.

The \textbf{RE} system, \textbf{MxTerminator} and \textbf{Punkt} were
described in Section~\ref{sec:previous-work}. The implementation of RE
used for our experiments was the one provided
here\footnote{http://www.ppgia.pucpr.br/âˆ¼silla/softwares/yasd.zip},
the implementation of MxTerminator was obtained from
here\footnote{ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz} and
the implementation of Punkt is the one from NLTK. As for training,
Punkt received the entire Brown corpus (training data and testing
data) without any annotations while MxTerm was trained using the
training data.

Punkt achieves remarkable performance and stands as the strongest
competitor to TrTok. They are both accurate language-independent tools
but TrTok's big shortcoming is its need for a corpus of manually
tokenized data.

\textbf{Apache OpenNLP} contains a sentence detector based around a
maximum entropy classifier. The implementation is nearly identical to
the specification of MxTerminator with only minor deviations (such as
signalling surrounding whitespace as features).

We performed experiments both with the ready-to-use sentence detection
model for English distributed via OpenNLP's
website\footnote{http://opennlp.sourceforge.net/models-1.5/en-sent.bin}
and with a model which was trained on our training data. The
parameters for optimally training the model were estimated using
10-fold cross-validation on the training data.

The \textbf{Stanford CoreNLP} sentence splitter works by applying its
tokenizer to the input text which will make the distinction between a
full stop as part of an abbreviation or an ordinal number as opposed
to being a sentence terminator. Thus the task of sentence splitting is
trivial after the tokenization has been correctly performed.

The tokenization is implemented using a lexical analyzer generator,
JFlex (not unlike how TrTok uses Quex to implement the
RoughTokenizer). It is a deterministic program guided by a collection
of heuristics.

The Stanford Tokenizer's performance is surprisingly good, considering
it has not had the chance to train itself on the target corpus.
However, the Stanford Tokenizer is written explicitly for English and
it is likely that its performance would not carry over to other
languages without significant work.

\textbf{TrTok::MxTerm-like} is a translation of the MxTerminator
method to TrTok. Whereas MxTerminator inspects the prefix and suffix
of the full-stop-containing word and the words preceding and
succeeding it, TrTok::MxTerm-like splits the period into its own rough
token and examines two rough tokens from both sides of it.

TrTox::MxTerm-like also nicely demonstrates the ease with which new
tokenization recipes can be defined in TrTok. The entire setup
consisted of creating a directory, collecting the abbreviations in a
single file and writing 5 lines of configuration, 2 or 3 of which
could be easily obsoleted by adopting saner defaults in TrTok and 1 of
which is purely for convenience.

The reason why MxTerminator does not achieve the same performance
could be that the maximum entropy trainer used in MxTerminator limits
itself to 100 iterations of Generalized Iterative Scaling, which
converges very slowly compared to L-BFGS \cite{maxent-algorithms}.
Another reason might be the fact that both MxTerminator and OpenNLP
cut off infrequent features.

The high performance of TrTok::MxTerm-like led us to try and see what
happens when we simplify the tokenization recipe even further, which
led to \textbf{TrTok::Easy} which works the same way as
TrTok::MxTerm-like, but it does not use any abbreviation lists, merely
the token types surrounding the full stop. Therefore, TrTok::Easy does
not rely on any external linguistic knowledge and is fairly language
universal, given that we have enough training data. The performance of
TrTok::Easy also points out that the difference in performance between
TrTok and MxTerminator/OpenNLP cannot be explained by the different
abbreviation lists.

Finally, \textbf{TrTok::Groomed} is a relatively large hand-made
tokenization setup. It includes 7 distinct lists of abbreviations
(prefix and suffix titles, abbreviated names of months\ldots)
totalling 303 types, 24 potential sentence terminators, possible
splits between characters from different classes, features for
detecting the case of tokens, for noticing numbers which happen to be
in the range of the days of the month or years, etc\ldots These
features are extracted from all rough tokens within 8 tokens from the
full stop and for the 2 closest tokens on either side of the full
stop, the token's type itself is passed as a feature.

Due to the large number of potential tokenization operations (and thus
rough tokens) and the host of user-defined features, TrTok::Groomed's
speed lags significantly behind the other TrTok recipes.

Interestingly, there is not much difference in the performance of
TrTok::Groomed, TrTok::MxTerm-like and TrTok::Easy. This tells us that
besides the token types in the close vicinity of the full stop, other
features are not that important. This highlights another use for TrTok
as a tool for the fast analysis of the importance of different
contextual features for performing the task of sentence detection.

\subsection{Chinese Word Segmentation}

Since TrTok is basically a general program for splitting text into
sequences (sentences) which are in turn composed of other sequences
(tokens) based on user-defined contextual features, TrTok can be used
for more than just sentence detection. One other segmentation task we
had hoped might be solvable using TrTok is Chinese word segmentation.

We ported the key features of one of the top contestants (which also
happens to employ a maximum entropy classifier)
\cite{seg-chinese-maxent} in the 2005 Second International Chinese
Word Segmentation Bakeoff into TrTok and evaluated its performance
using the bakeoff evaluation scripts. The resulting segmenter achieved
a median performance with respect to the other contestants of the
Bakeoff.
