\section{Introduction}
\label{sec:introduction}

Tokenization and segmentation are parts of almost every natural
language processing system, since most of the higher-level language
processing applications assume we are dealing with words and
sentences, not with streams of bytes representing characters.


Segmentation (also referred to as sentence detection or sentence
boundary disambiguation) has been tackled using a variety of
techniques. The most common approaches include writing heuristics and
constructing abbreviation lists (the Stanford Tokenizer, the RE
system) or using machine learning algorithms to predict the role of a
potential sentence terminator (Satz, MxTerminator, Apache OpenNLP).
There have also recently been some very successful systems using
unsupervised methods (Punkt).

Tokenization is a problem which stops being trivial when we start
considering whitespace-free languages such as Chinese or Japanese. In
these languages, tokenization (also referred to as word segmentation)
receives a lot of attention \cite{seg-bakeoff}.

TrTok aims to be a practical tool for tokenizing and segmenting text
written in any language. To achieve such a goal, TrTok relies on the
user determining the specifics of training and tokenization and
providing the necessary training data.

TrTok's novelty comes in its openness and formalization of the
tokenization process. The process is divided into several discrete
stages, most of which are heavily customizable. The user is able to
say where in the text should TrTok consider breaking up or joining
tokens or sentences, how should TrTok represent the context of these
decision points to the underlying classifier, how should the
classifier be trained, how should existing whitespace be treated,
etc\ldots TrTok was also built to be a practical tool, which means it
can transparently process text interspersed with XML markup and HTML
entities and was designed to run fast.

The major inconveniences of TrTok are that due to its customizability
it needs to be properly set up and due to its reliance on machine
learning methods, it requires manually tokenized training data.
Furthermore, its dependency on external tools such as a build system
for automation of runtime code compilation can make it nontrivial to
deploy.
