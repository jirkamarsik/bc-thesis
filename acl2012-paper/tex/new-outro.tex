\section{Conclusion}
\label{sec:outro}

We have presented and described a universal tool for segmenting and
tokenizing textual data. We have applied the tool to detecting
sentences in English text and identifying words in Chinese text. We
have shown that in both cases, TrTok can offer performance which is
competitive with previous approaches, more so in the case of English
sentence detection, where TrTok outperformed existing implementations.

Since TrTok lets us define a lot of its behavior using declarative
rules and feature descriptions, it might be interesting to harness
this ability to find out the effect of various contextual cues on the
performance of a sentence detector.

On the software side of things, TrTok would also benefit from getting
more user-friendly, which would entail providing a walkthrough of the
setup process, distributing example setups and trained models and
offering an all-dependencies-included compiled package for easier
deployment.
