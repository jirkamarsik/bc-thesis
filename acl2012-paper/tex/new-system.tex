\section{Description of the System}
\label{sec:system}

TrTok is implemented by a parallel execution of several configurable
pipeline steps.

\subsection{TextCleaner}

The first pipeline element, the TextCleaner, reads in and decodes the
input. Additionally, it can strip XML markup from the input and expand
HTML entities to their character counterparts. If the TextCleaner does
so, it can be also configured to relay these changes to the final
pipeline element, the OutputFormatter, which can effectively undo
these changes by reinserting the XML markup and the HTML entities in
the correct position in the output.

\subsection{RoughTokenizer}

The RoughTokenizer stage partitions the stream of characters from the
TextCleaner into small, discrete chunks of non-blank characters,
called rough tokens. The partitioning can be made more granular by
user-defined rules which specify positions at which the desired
tokenization might differ from the whitespace-induced one.

A location in the text may be marked as a \maysplit{} meaning that the
characters in the text preceding and following it may be parts of
different tokens even though they are not separated by whitespace
(e.g. we might wish to put a \maysplit{} between \example{``was''} and
\example{``n't''} in \example{``wasn't''}).

A location within a span of white characters might be labeled as a
\mayjoin{} signalling that the characters preceding and following the
whitespace area might be parts of the same token, as in the case of
spaces entered in long numerals for readability (e.g.
\example{``12~345''}).

Finally, a location in the text may be marked as a \maybreaksentence{}
if the characters preceding it and the characters following it might
belong to different sentences. These locations traditionally occur
after possible sentence terminators, but can include some
domain-specific contexts as well (e.g. line breaks (possibly preceded
by commas) when segmenting poetry).

The placement of the \maysplit{}, \mayjoin{} and \maybreaksentence{}
events further informs the rough tokenization which cuts the rough
tokens so that no rough token contains an event within its boundaries.
Note that the presence of a \may{} event only signifies the
possibility of a tokenization operation (splitting or joining of
tokens or sentences). Whether a token split, token join or sentence
break will occur is up to the Classifier.

As for how the locations of these possible tokenization operations are
determined, each type of event is associated with a set of rules. A
rule consists of a pair of regular expressions, $l$ and $r$. An event
is fired off at a location if a suffix of the text preceding it
matches $l$ and a prefix of the text succeeding matches $r$. All of
the events are handled in this uniform manner with the exception that
the \mayjoin{} patterns also implicitly match the whitespace span
separating the rough tokens.

The most challenging aspect of developing TrTok was to make sure that
the system would be able to correctly identify all of the above events
given any input regular expressions and to do so fast. TrTok solves
this problem by using the lexical analyzer generator Quex (a fast and
more Unicode-friendly variation of the classic tools lex and flex).

On launch, TrTok checks if the files defining the rules for rough
tokenization have changed since last time and if so, constructs a Quex
program by inserting the user-specified regular expressions into a
prepared template. The resulting program is first run through Quex
which turns it into a fast C++ implementation of a FSM which emits
tokens signalling the events in question and transmitting the text in
between them. This code is then compiled, stored for later reuse and
dynamically loaded in. The RoughTokenizer then interfaces with this
code and uses it to process the input it gets from the TextCleaner.

\subsection{FeatureExtractor}

The stream of rough tokens interleaved with potential tokenization
operations output by the RoughTokenizer is processed using the
FeatureExtractor. The FeatureExtractor simply annotates each rough
token with a bit vector signifying which of the user-defined features
hold for the rough token in question.

The features can be defined in two ways: either using a regular
expression or a list of rough tokens. If a feature is defined using a
regular expression, a rough token is said to have that feature iff the
regular expression matches the entire rough token. In the case of a
feature defined using a list of rough tokens, a rough token is said to
have the feature iff it is on the list.

This way it is easy to specify features which try to analyze the shape
of rough tokens using regular expressions or to simply give a list of
all interesting tokens (e.g. words of a certain part of speech or
exceptions such as abbreviations).

\subsection{Classifier}

The Classifier is the other ``hard worker'' of the system (besides the
RoughTokenizer). Its job is to disambiguate the potential tokenization
operations identified by the RoughTokenizer, i.e. it decides whether a
\maysplit{} truly splits a word into two tokens, whether a \mayjoin{}
joins two words into one token and whether a \maybreaksentence{} truly
ends a sentence. It does so by consulting a maximum entropy classifier
for every location containing these potential tokenization operations.

The features passed to the classifier consist of the features of words
in the context surrounding the potential tokenization operation and
the presence of whitespace and potential tokenization operations in
the context area. The user is free to select the size of the context
area and which features from which words in the context area are to be
passed to the classifier.

The maximum entropy module then classifies the instance as either a
SENTENCE\_BOUNDARY, a TOKEN\_BOUNDARY or simply NO\_BOUNDARY. The
Classifier uses this outcome to disambiguate the potential
tokenization operations.

\subsection{OutputFormatter}

The OutputFormatter is the point at which the stream of rough tokens
is turned back into a character stream. This means that all the rough
tokens are concatenated and whitespace is inserted between them
depending on whether there originally was any whitespace between them
and on the tokenization operations which are to be carried out in the
space between them.

Individual tokens end up being separated by a single space character
and sentences are separated by line breaks. Furthermore, instances of
multiple consequent line breaks in the input can be optionally
preserved in the output, which is useful if they were used to separate
the input document into different subdocuments.

On top of that, it can also be the duty of the OutputFormatter to
reinsert any XML markup which was originally present in the text and
to encode characters which originally were HTML entities back into the
same HTML entities. Both of these normalization transformations are
supplied so that idiosyncrasies of the coding and any XML metadata do
not interfere with the feature extraction or token boundary
classification.

\subsection{Encoder}

The final stage of the process is the Encoder whose only
responsibility is to convert the internal UTF\-8-encoded character
stream into the target encoding and to send it to the program's output.

\subsection{Modes of Operation}

The pipeline described above applies when we are using TrTok to
tokenize data. However, before we can do that, we need to train a
classifier for the Classifier stage. For this purpose, TrTok offers
another mode of operation for training which uses a collection of raw
input data paired with the manually tokenized and segmented data to
train the model.

Besides these two core modes, TrTok also includes a mode for
tokenizing data and then comparing the results with manually annotated
data and outputting the difference so that its performance can be
evaluated. Finally, TrTok can also ``tokenize'' a file without a
classifier, choosing a sentence or token boundary wherever possible to
prepare data more suitable for manual annotation.

\subsection{Tokenization Schemes}

The configuration of the system (the behavior of the pipeline stages
all the way from the TextCleaner to the OutputFormatter) is called a
\newterm{tokenization scheme}. TrTok is built with support for a
hierarchical organization of tokenization schemes where tokenization
schemes lower in the hierarchy share the configuration files of those
higher in the hierarchy unless they provide their own version. This
cuts down duplication in the configuration of the tool for different
languages or contexts and makes it possible to tweak an existing
scheme without modifying it by creating a subscheme.
