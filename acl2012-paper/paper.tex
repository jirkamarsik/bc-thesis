\documentclass[11pt]{article}

\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}

\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{gnuplottex}

\usepackage{mystyle}

\usepackage[ps2pdf,unicode]{hyperref}   % Musí být za všemi ostatními balíčky

\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{TrTok: A Fast and Trainable Tokenizer for Natural Languages}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
We present a universal data-driven tool for segmenting and tokenizing
text. The presented tokenizer lets the user define where token and
sentence boundaries should be considered. These instances are then
judged by a classifier which is trained from provided tokenized data.
The features passed to the classifier are also defined by the user.
This level of customizability makes the tokenizer a versatile tool
which we show is capable of sentence detection in English text as well
as word segmentation in Chinese text. In the case of English sentence
detection, the system outperformed previous methods.
\end{abstract}

\input{tex/new-intro}
\input{tex/new-previous-work}
\input{tex/new-system}
\input{tex/new-eval}
\input{tex/new-outro}

\bibliographystyle{acl2012}
\bibliography{sbd,maxent,seg,web,data}

\end{document}
