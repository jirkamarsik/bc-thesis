\documentclass[11pt]{article}

\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}

\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{gnuplottex}

\usepackage{mystyle}

\usepackage[ps2pdf,unicode]{hyperref}   % Musí být za všemi ostatními balíčky

\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{TrTok: A Fast and Trainable Tokenizer for Natural Languages}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
We present a data-driven tool for disambiguating token and sentence
boundaries. The implemented system is highly configurable and
versatile to the point its tokenization abilities allow to segment
unbroken Chinese text. The tokenizer relies on maximum entropy
classifiers and requires a sample of tokenized and segmented text as
training data. The system was built with multi-platform libraries only
and with emphasis on speed and correctness. After a necessary survey
of other tools for text tokenization and segmentation and a short
introduction to maximum entropy modelling, a large part of the paper
focuses on the particular implementation we developed and its
evaluation.
\end{abstract}

\input{tex/new-intro}
\input{tex/new-previous-work}
\input{tex/new-system}
\input{tex/new-eval}
\input{tex/new-outro}

\bibliographystyle{acl2012}
\bibliography{sbd,maxent,seg,web,data}

\end{document}
