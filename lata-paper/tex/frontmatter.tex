\nobreak\vbox to 0.49\vsize{
\setlength\parindent{0mm}
\setlength\parskip{3mm}

Title:
Fast and Trainable Tokenizer for Natural Languages

Author:
Jiří Maršík

Supervisor:
RNDr. Ondřej Bojar Ph.D.

Abstract:
In this thesis, we present a data-driven system for disambiguating token and
sentence boundaries. The implemented system is highly configurable and
versatile to the point its tokenization abilities allow to segment unbroken
Chinese text. The tokenizer relies on maximum entropy classifiers and requires
a sample of tokenized and segmented text as training data. The program is
accompanied by a tool for reporting the performance of the tokenization which
helps to rapidly develop and tune the tokenization process. The system was
built with multi-platform libraries only and with emphasis on speed and
correctness. After a necessary survey of other tools for text tokenization and
segmentation and a short introduction to maximum entropy modelling, a large
part of the thesis focuses on the particular implementation we developed and
its evaluation.

Keywords:
tokenization, segmentation, maximum entropy, text preprocessing

\vss}

\newpage
