\section{Evaluation}
\label{sec:eval}

We evaluated our implementation of TrTok compared to a wide range of
prominent implementations and approaches to sentence detection. The
results are presented in Table~\ref{tbl:grand-melee}.

\begin{table}
  \small
  \begin{center}
    \begin{tabular}{ | l | c | c | c | c | c | r | }
      \hline
      & Acc. $\downarrow$ & Err. Rate & Prec.
      & Recall & F_1 & Time \\ \hline
      TrTok::Groomed & \textbf{98.86\%} & \textbf{1.14\%} & \textbf{99.12\%}
                     & 99.57\% & \textbf{99.34\%} & 5.10s \\ \hline
      Stanford CoreNLP & 98.83\% & 1.17\% & 98.78\%
                       & 99.89\% & 99.33\% & 5.02s \\ \hline
      TrTok::MxTerm-like & 98.76\% & 1.24\% & 98.70\%
                         & 99.89\% & 99.29\% & 1.10s \\ \hline
      TrTok::Easy & 98.70\% & 1.30\% & 98.61\%
                  & 99.91\% & 99.26\% & 1.08s \\ \hline
      Punkt & 98.65\% & 1.35\% & 98.82\%
            & 99.63\% & 99.22\% & 3.13s \\ \hline
      MxTerminator & 98.27\% & 1.73\% & 98.30\%
                   & 99.74\% & 99.01\% & 1.37s \\ \hline
      Apache OpenNLP & 98.20\% & 1.80\% & 98.20\%
                     & 99.77\% & 98.97\% & 1.13s \\ \hline
      Apache OpenNLP (ready) & 97.71\% & 2.29\% & 98.62\%
                             & 98.75\% & 98.68\% & 1.17s \\ \hline
      RE & 97.26\% & 2.74\% & 98.52\%
         & 98.32\% & 98.42\% & 16.93s \\ \hline
      TrTok::Satz-like & 96.50\% & 3.50\% & 97.91\%
                       & 98.08\% & 97.99\% & 1.59s \\ \hline
      TrTok::Baseline & 91.84\% & 8.16\% & 91.67\%
                      & 99.66\% & 95.50\% & 0.85s \\ \hline
      Absolute Baseline & 86.89\% & 13.11\% & 86.89\%
                        & \textbf{100.00\%} & 92.99\% & \textbf{0.02s} \\ \hline
    \end{tabular}
  \end{center}
  \caption[Performance of sentence detectors on the Brown corpus] {The
    performance of the various sentence detectors on full stops from
    the Brown corpus testing data. The 1.15 MB of testing data
    consisted of 11376 sentences and 232893 tokens.}
  \label{tbl:grand-melee}
\end{table}

\subsection{Dataset}

The experiments were conducted on the Brown corpus \cite{data-brown}
as is supplied through NLTK \cite{software-nltk}. A representative
(covering each category of text proportionately) 20\% of the corpus
was used as the testing data. This number was chosen so that the
testing data would be sure to contain at least 1000 instances of a
non-sentence-terminating full stop; the resulting test set ended up
containing 1481 such full stops. The rest of the data was made
available for training to the supervised learning methods.

\subsection{Performance Measurement}

The performance of the evaluated systems was measured by their success
(accuracy) in classifying instances of the full stop character. The
text contains other sentence terminators such as the question mark and
the exclamation mark, but they almost never figure as anything else
but sentence terminators in the text. Other occasional sentence
delimiters such as dashes, semicolons, colons and line breaks were
ignored as well, since the other systems usually do not have a
solution for them. This way the comparison is fair. Furthermore, the
full stop is the most common and ambiguous of the sentence delimiters,
so it makes sense to focus on it.

Besides the systems' accuracy, we also measure the time spent
processing the testing data and we present the median of 11 runs to
bring the implementation speed of the systems into context as well.

\subsection{Sentence Detection Methods}

\textbf{Absolute Baseline} simply marks every full stop as a sentence
terminator.

\textbf{Trtok::Baseline} is the simplest tokenizer which can be
written in TrTok. The reason that this performs noticeably better than
the Absolute Baseline is that even if we do not specify any contextual
features explicitly, TrTok still passes the built-in feature
\texttt{0:\%WHITESPACE} to the classifier, which tells it whether the
full stop in question is followed by any whitespace or not. The
classifier has then learned that periods not followed by whitespace
usually do not mean the end of a sentence and uses that to perform
better than the Absolute Baseline.

\textbf{TrTok::Satz-like} is a straightforward attempt at translating
the Satz system to TrTok. The POS-tagged training data was used to
construct lexicons for each different part of speech tag (NLTK's
method of simplifying tags was used to reduce the number of different
tags to help fight data sparsity). The classifier was then given all
the retrieved part of speech tags for the 3 rough tokens preceding and
following a full stop.

In contrast to Satz, our version uses a different system of tags, a
different machine learning algorithm and most importantly our version
does not try to guess the part of speech tags for words which are not
found in the lexicon. All of these decisions are for the worse (except
maybe for the choice of machine learning algorithm and hypothesis
representation) and so TrTok::Satz-like does not perform as well as
the original Satz system would have \cite{sbd-satz}. Sadly, the
implementation of Satz is no longer in the author's possession.

The \textbf{RE} system, \textbf{MxTerminator} and \textbf{Punkt} were
described in Section~\ref{sec:previous-work}. The implementation of RE
used for our experiments was the one provided
here\footnote{http://www.ppgia.pucpr.br/âˆ¼silla/softwares/yasd.zip},
the implementation of MxTerminator was obtained from
here\footnote{ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz} and
the implementation of Punkt is the one from NLTK. As for training,
Punkt received the entire Brown corpus (training data and testing
data) without any annotations while MxTerm was trained using the
training data.

Punkt achieves remarkable performance and stands as the strongest
competitor to TrTok in the field of multilingual sentence detection.
They are both accurate language-independent tools but TrTok's big
shortcoming is its need for a corpus of manually tokenized data.

\textbf{Apache OpenNLP} contains a sentence detector based around a
maximum entropy classifier. The implementation is nearly identical to
the specification of MxTerminator with only minor deviations (such as
signalling surrounding whitespace as features).

We performed experiments both with the ready-to-use sentence detection
model for English distributed via OpenNLP's
website\footnote{http://opennlp.sourceforge.net/models-1.5/en-sent.bin}
and with a model which was trained on our training data. The
parameters for optimally training the model were estimated using
10-fold cross-validation on the training data.

The \textbf{Stanford CoreNLP} sentence splitter works by applying its
tokenizer to the input text which will make the distinction between a
full stop as part of an abbreviation or an ordinal number as opposed
to being a sentence terminator. Thus the task of sentence splitting is
trivial after the tokenization has been correctly performed.

The tokenization is implemented using a lexical analyzer generator,
JFlex (not unlike how TrTok uses Quex to implement the
RoughTokenizer). It is a deterministic program guided by a collection
of heuristics.

The Stanford Tokenizer's performance is excellent, especially
considering it has not had the chance to train itself on the target
corpus. However, the Stanford Tokenizer is written explicitly for
English and it is likely that its performance would not carry over to
other languages without significant work.

\textbf{TrTok::MxTerm-like} is a translation of the MxTerminator
method to TrTok. Whereas MxTerminator inspects the prefix and suffix
of the full-stop-containing word and the words preceding and
following it, TrTok::MxTerm-like splits the period into its own rough
token and examines two rough tokens on both sides of it.

TrTox::MxTerm-like also nicely demonstrates the ease with which new
tokenization setups can be defined in TrTok. The entire setup
consisted of creating a directory, collecting the abbreviations in a
single file and writing 5 lines of configuration, 2 or 3 of which
could be easily obsoleted by adopting saner defaults in TrTok and 1 of
which is purely for convenience.

The reason why MxTerminator does not achieve the same performance
could be that the maximum entropy trainer used in MxTerminator limits
itself to 100 iterations of Generalized Iterative Scaling, which
converges very slowly compared to L-BFGS \cite{maxent-algorithms}.
Another reason might be the fact that both MxTerminator and OpenNLP
cut off infrequent features.

The high performance of TrTok::MxTerm-like led us to try and see what
happens when we simplify the tokenization setup even further, which
led to \textbf{TrTok::Easy} which works the same way as
TrTok::MxTerm-like, but it does not use any abbreviation lists, merely
the token types surrounding the full stop. Therefore, TrTok::Easy does
not rely on any external linguistic knowledge and is fairly language
universal, given that we have enough training data. The performance of
TrTok::Easy also points out that the difference in performance between
TrTok and MxTerminator/OpenNLP cannot be explained by the different
abbreviation lists.

Finally, \textbf{TrTok::Groomed} is a relatively large hand-made
tokenization setup ported from a previous version of the tokenizer. It
considers 24 different potential sentence terminators, it includes 7
distinct lists of abbreviations totalling 303 types (prefix and suffix
titles, abbreviated names of months\ldots) and it implements features
for detecting the case of tokens, for noticing numbers which happen to
be in the range of years or the days of the month, etc\ldots\ \ These
features are extracted from rough tokens within 8 tokens distance from
the full stop. The 2 closest tokens on either side of the full stop
also contribute their token type as a feature.

Due to the large number of potential tokenization operations (and thus
rough tokens) and the host of user-defined features, TrTok::Groomed's
speed lags significantly behind the other TrTok setups.

Interestingly, there is not much difference in the performance of
TrTok::Groomed, TrTok::MxTerm-like and TrTok::Easy. This tells us that
besides the token types in the close vicinity of the full stop, other
features are not that important. This highlights another use for TrTok
as a tool for the fast analysis of the importance of different
contextual features for performing the task of sentence detection.

\subsection{Chinese Word Segmentation}

Since TrTok is basically a general program for splitting text into
sequences (sentences) which are in turn composed of other sequences
(tokens) based on user-defined contextual features, TrTok can be used
for more than just sentence detection. One other segmentation task we
had hoped might be solvable using TrTok is Chinese word segmentation.

We ported the key features of one of the top contestants (which also
happens to employ a maximum entropy classifier)
\cite{seg-chinese-maxent} in the 2005 Second International Chinese
Word Segmentation Bakeoff into TrTok and evaluated its performance
using the bakeoff evaluation scripts. The results achieved (see
Table~\ref{tbl:bakeoff-score}) are approximately a median of the
scores reached by submissions to the Bakeoff.

\begin{table}
  \begin{center}
    \begin{tabular}{ | l | r | r | r | r | }
      \hline
      & True Words Recall & Test Words Precision & F-measure \\ \hline
      Academia Sinica & 0.933 & 0.919 & 0.926 \\ \hline
      City University & 0.934 & 0.934 & 0.934 \\ \hline
      Peking University & 0.923 & 0.933 & 0.928 \\ \hline
      Microsoft Research & 0.951 & 0.952 & 0.951 \\
      \hline
    \end{tabular}
  \end{center}
  \caption[Chinese Word Segmentation scores]
    {The scores assigned to our tokenizer by the official scoring script of the
    Second International Chinese Word Segmentation Bakeoff, sorted by dataset.}
  \label{tbl:bakeoff-score}
\end{table}

